import { UnifiedChatCase } from '@/types/chat';
import { DocumentArtifact } from '@/types/analyzer';
import { OCRResult } from '@/types/ocr';
import { DocumentClassifier } from '@/lib/ocr/classify_doc';
import { ClaimFieldExtractor } from '@/lib/ocr/extract_claim_fields';
import { PDFTextDetector } from '@/lib/ocr/detect_pdf_text';
import { CloudOCRService } from '@/lib/ocr/cloud_ocr';
import { LocalOCRService } from '@/lib/ocr/local_ocr';
import { NoBenefitsDetectionEngine } from '@/lib/detect/engine';
import { TableOutputFormatter } from '@/lib/formatters/table_formatter';

export class ChatImageProcessor {
  private pdfDetector = new PDFTextDetector();
  private cloudOCR = new CloudOCRService();
  private localOCR = new LocalOCRService();
  private classifier = new DocumentClassifier();
  private fieldExtractor = new ClaimFieldExtractor();
  private detectionEngine = new NoBenefitsDetectionEngine();
  private formatter = new TableOutputFormatter();

  /**
   * Process uploaded image/document for chat integration
   * Returns structured data and user-friendly analysis
   */
  public async processUploadedDocument(
    buffer: Buffer,
    filename: string,
    mimeType: string,
    userMessage?: string
  ): Promise<UnifiedChatCase> {
    console.log(`üñºÔ∏è Processing uploaded document: ${filename} (${mimeType})`);

    try {
      // Step 1: Extract text using hybrid OCR pipeline
      const ocrResult = await this.extractTextWithHybridPipeline(buffer, mimeType);

      // Step 2: Classify document type
      const classification = this.classifier.classifyDocument(buffer, filename, mimeType, ocrResult);

      // Step 3: Extract structured fields
      const fullText = ocrResult.tokens.map(token => token.text).join(' ');
      const extractedFields = await this.fieldExtractor.extractFields(fullText, classification.docType, ocrResult);

      // Step 4: Create document artifact
      const documentArtifact: DocumentArtifact = {
        artifactId: `artifact_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
        filename: filename,
        mime: mimeType,
        docType: classification.docType,
        pages: 1
      };

      // Step 5: Run no-benefits detection (would be implemented with proper context)
      const detectionResults: any[] = [];

      // Step 6: Format results for user consumption
      const formattedLineItems = this.formatter.formatLineItems(
        extractedFields.lineItems,
        classification.confidence
      );

      const formattedDetections = this.formatter.formatDetections(detectionResults);

      const formattedFinancials = this.formatter.formatFinancialSummary({
        billed: 0,
        allowed: 0,
        planPaid: 0,
        patientResp: 0
      });

      // Step 7: Generate analysis summary
      const analysisSummary = this.generateAnalysisSummary(
        documentArtifact,
        detectionResults,
        formattedDetections
      );

      // Step 8: Create unified chat case
      const chatCase: UnifiedChatCase = {
        caseId: `case_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
        artifacts: [documentArtifact],
        narrative: {
          text: userMessage || `Please analyze this ${classification.docType.toLowerCase()} document`,
          themeHints: []
        }
      };

      console.log(`‚úÖ Document processing complete - ${classification.docType} with ${detectionResults.length} issues detected`);

      return chatCase;

    } catch (error) {
      console.error('‚ùå Document processing failed:', error);

      // Return error case
      return {
        caseId: `error_case_${Date.now()}`,
        artifacts: [],
        narrative: {
          text: userMessage || 'Document analysis failed',
          themeHints: []
        }
      } as UnifiedChatCase;
    }
  }

  /**
   * Hybrid OCR pipeline: vector text ‚Üí cloud ‚Üí local fallback
   */
  private async extractTextWithHybridPipeline(buffer: Buffer, mimeType: string): Promise<OCRResult> {
    // Step 1: Try vector text extraction for PDFs
    if (mimeType === 'application/pdf') {
      try {
        const pdfTextResult = await this.pdfDetector.extractText(buffer);
        if (pdfTextResult.hasExtractableText && pdfTextResult.blocks.length > 0) {
          console.log('‚úÖ Using vector text extraction');
          return this.convertPDFTextToOCRResult(pdfTextResult);
        }
      } catch (error) {
        console.warn('‚ö†Ô∏è Vector text extraction failed:', error);
      }
    }

    // Step 2: Try cloud OCR services
    try {
      const cloudResult = await this.cloudOCR.processDocument(buffer, mimeType);
      console.log(`‚úÖ Using cloud OCR: ${cloudResult.metadata.engine}`);
      return cloudResult;
    } catch (error) {
      console.warn('‚ö†Ô∏è Cloud OCR failed:', error);
    }

    // Step 3: Fallback to local Tesseract
    console.log('üè† Falling back to local OCR');
    return await this.localOCR.processDocument(buffer, mimeType);
  }

  private convertPDFTextToOCRResult(pdfTextResult: any): OCRResult {
    // Convert PDF text blocks to OCR token format
    const tokens = pdfTextResult.blocks.map((block: any, index: number) => ({
      text: block.text,
      bbox: block.bbox,
      conf: 0.95, // High confidence for vector text
      page: block.page
    }));

    return {
      tokens,
      kvs: [], // Would need layout analysis to extract key-value pairs
      tables: [], // Would need table detection
      metadata: {
        engine: 'vector',
        pages: pdfTextResult.pages,
        docTypeHint: 'pdf_text'
      }
    };
  }

  private generateAnalysisSummary(
    artifact: DocumentArtifact,
    detectionResults: any[],
    formattedDetections: any
  ): string {
    const triggeredDetections = detectionResults.filter(r => r.triggered);
    const highSeverityCount = formattedDetections.highSeverityCount;
    const totalSavings = formattedDetections.totalPotentialSavings;

    let summary = `## Document Analysis Summary\n\n`;

    // Document type and confidence
    summary += `**Document Type:** ${artifact.docType}\n\n`;

    // Line items summary (would use extracted data in real implementation)
    summary += `**Processing Status:** Document successfully analyzed\n\n`;

    // Detection results
    if (triggeredDetections.length === 0) {
      summary += `‚úÖ **No billing issues detected** - This document appears to follow standard billing practices.\n\n`;
    } else {
      summary += `‚ö†Ô∏è **${triggeredDetections.length} potential issues detected**\n\n`;

      if (highSeverityCount > 0) {
        summary += `üî¥ ${highSeverityCount} high-severity issues require immediate attention\n\n`;
      }

      if (totalSavings > 0) {
        summary += `üí∞ Potential savings: $${(totalSavings / 100).toFixed(2)}\n\n`;
      }

      // Top issues
      const topIssues = formattedDetections.topDetections.slice(0, 3);
      if (topIssues.length > 0) {
        summary += `**Key Issues:**\n`;
        topIssues.forEach((issue: any, index: number) => {
          summary += `${index + 1}. **${issue.name}:** ${issue.message}\n`;
        });
        summary += `\n`;
      }
    }

    // Analysis completed
    summary += `üîç **Analysis Complete:** Ready for review and action\n\n`;

    // Next steps
    summary += `**Recommended Actions:**\n`;
    if (triggeredDetections.length > 0) {
      summary += `‚Ä¢ Review the detailed detection results below\n`;
      summary += `‚Ä¢ Verify supporting documentation for flagged items\n`;
      summary += `‚Ä¢ Contact provider for clarification on questionable charges\n`;
    } else {
      summary += `‚Ä¢ Document appears compliant with billing standards\n`;
      summary += `‚Ä¢ Consider this analysis for your records\n`;
    }

    return summary;
  }
}